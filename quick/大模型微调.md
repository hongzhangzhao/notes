

# 1 微调步骤

- 选择微调任务
- 寻找基础模型
- 准备数据集
- lora微调
- 评估模型


# 2 准备数据集

- 大模型的数据集格式一般包含：指令、输入、输出和text，可以参考下面的地址：
    - https://huggingface.co/datasets/vicgalle/alpaca-gpt4
    - text的内容就是将指令、输入和输出的内容整合到一起，并用特殊符号进行区分

## 2.1 chatGPT生成数据集

生成数据集的指令

- Based on that, create a dataframe with concept and description, including 25 examples of concept and description pairs
- Good, please give me another 30 unique pairs

```py
import pandas as pd

data = {
    'Concept': [
        'Machine Learning',
        'Artificial Intelligence',
        'Neural Networks',
        'Data Mining'
    ],
    'Description': [
        'A branch of artificial intelligence (AI) that enables machines to learn from data without being explicitly programmed.',
        'The simulation of human intelligence processes by machines, especially computer systems.',
        'A set of algorithms, modeled loosely after the human brain, that are designed to recognize patterns.',
        'The process of discovering patterns in large data sets involving methods at the intersection of machine learning, statistics, and database systems.'
    ]
}

df = pd.DataFrame(data)

df.to_csv('my_data.csv', index=False)
# df.to_excel('my_data.xlsx', index=False)
# df.to_json('my_data.json')
```


### 2.1.1 构造text内容

生成构造text的代码指令

```
I have a csv file uploaded, please give me the code to achieve the following task create another column called text which follows the following structure:
test=f"###Human:\ngenerate a stable diffusion prompt for
{Concept)n\n###Assistant\n{Description|"
```

可以看到使用了###这三个符号区分了人类的指令和机器的回答

```py
import pandas as pd

df = pd.read_csv('my_data.csv')

df['text'] = df.apply(lambda x: f"""###Human:
generate a stable diffusion prompt for {x['concept']}

### Assistant:
{x['description']}""",axis=1)

df.to_csv('train.csv', index=False)
```


# 3 程序步骤

## 3.1 安装依赖

```py
!pip install -U trl transformers accelerate git+https://github.com/huggingface/peft.git
!pip install datasets bitsandbytes einops wandb

pip install -U  trl git+https://github.com/huggingface/peft.git
pip install datasets bitsandbytes wandb
```

## 3.2 加载数据集

加载数据集分为：加载fugging face上的和本地的

```py
from datasets import load_dataset

# dataset_name = 'vicgalle/alpaca-gpt4'
# dataset = load_dataset(dataset_name, split="train")

# 加载本地的CSV文件作为数据集
dataset = load_dataset('csv', data_files='train。csv')

# 打印数据集的前几行
print(dataset['train'][:5])
```

## 3.3 加载模型

```py
import torch
from transformers import AutoModelForCausalLM, AutoTokenizer, BitsAndBytesConfig

model_name = "/model/Qwen-7B-Chat"

bnb_config = BitsAndBytesConfig(
        load_in_4bit=True,
        bnb_4bit_quant_type="nf4",
        bnb_4bit_compute_dtype=torch.float16,
    )

model = AutoModelForCausalLM.from_pretrained(
        model_name,
        quantization_config=bnb_config,
        trust_remote_code=True
    )

model.config.use_cache = False

```

## 3.4 加载分词器

```py

tokenizer = AutoTokenizer.from_pretrained(model_name, trust_remote_code=True)
tokenizer.pad_token = tokenizer.eos_token
```

## 3.5 初始化lora参数

```py

from peft import LoraConfig, get_peft_model

lora_alpha = 16
lora_dropout = 0.1
lora_r = 64

peft_config = LoraConfig(
    lora_alpha = lora_alpha,
    lora_dropout=lora_dropout,
    r=lora_r,
    bias="none",
    task_type="CAUSAL_LM"
    )
```

## 3.6 加载训练器（SFTTrainer）

```py

xxx

```

# 4 微调工具

kohya 训练器 （训练程序） 俗称 “丹炉”

# 5 案例

- 案例：https://github.com/ShawhinT/YouTube-Blog
    - 作者的youtube频道：https://www.youtube.com/@ShawhinTalebi
- https://github.com/ForestDake/RAG_simp_DEMO

## 5.1 .ipynb to .py

```py
pip install jupyter
jupyter nbconvert --to script xxx.ipynb
```


# 6 其它

## 6.1 运行 qwen-7b-chat

### 6.1.1 安装依赖

```
# hugging face上有使用说明
pip install transformers==4.32.0 accelerate tiktoken einops scipy transformers_stream_generator==0.0.4 peft deepspeed -i https://pypi.tuna.tsinghua.edu.cn/simple

# 下载github上有开源项目来安装依赖 https://github.com/QwenLM/Qwen
pip install  -r requirements.txt -i https://pypi.tuna.tsinghua.edu.cn/simple
```


# 7 parquet文件


pip install parquet-tools

parquet-tools head local-file | less





